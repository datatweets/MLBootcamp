{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals\n",
    "\n",
    "Welcome to this comprehensive guide on Machine Learning fundamentals! This notebook will cover the essential concepts, algorithms, and practical implementations to get you started with ML.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Machine Learning](#introduction)\n",
    "2. [Types of Machine Learning](#types)\n",
    "3. [Data Preprocessing](#preprocessing)\n",
    "4. [Linear Regression](#linear-regression)\n",
    "5. [Logistic Regression](#logistic-regression)\n",
    "6. [Decision Trees](#decision-trees)\n",
    "7. [K-Nearest Neighbors (KNN)](#knn)\n",
    "8. [Model Evaluation](#evaluation)\n",
    "9. [Cross-Validation](#cross-validation)\n",
    "10. [Feature Engineering](#feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_iris, load_boston, make_classification, make_regression\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='introduction'></a>\n",
    "## 1. Introduction to Machine Learning\n",
    "\n",
    "**Machine Learning** is a subset of Artificial Intelligence that enables computers to learn from data without being explicitly programmed.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Features (X)**: Input variables used to make predictions\n",
    "- **Target (y)**: Output variable we want to predict\n",
    "- **Model**: Mathematical representation that learns patterns from data\n",
    "- **Training**: Process of teaching the model using data\n",
    "- **Prediction**: Using the trained model to make forecasts on new data\n",
    "\n",
    "### The ML Workflow:\n",
    "1. **Data Collection**: Gather relevant data\n",
    "2. **Data Preprocessing**: Clean and prepare data\n",
    "3. **Feature Engineering**: Select and create meaningful features\n",
    "4. **Model Selection**: Choose appropriate algorithm\n",
    "5. **Training**: Fit the model to training data\n",
    "6. **Evaluation**: Assess model performance\n",
    "7. **Optimization**: Fine-tune parameters\n",
    "8. **Deployment**: Use model in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='types'></a>\n",
    "## 2. Types of Machine Learning\n",
    "\n",
    "### 2.1 Supervised Learning\n",
    "Learning from labeled data (input-output pairs)\n",
    "- **Regression**: Predicting continuous values (e.g., house prices)\n",
    "- **Classification**: Predicting discrete categories (e.g., spam/not spam)\n",
    "\n",
    "### 2.2 Unsupervised Learning\n",
    "Learning from unlabeled data\n",
    "- **Clustering**: Grouping similar data points\n",
    "- **Dimensionality Reduction**: Reducing feature space\n",
    "\n",
    "### 2.3 Reinforcement Learning\n",
    "Learning through interaction with an environment (rewards/penalties)\n",
    "\n",
    "**This notebook focuses on Supervised Learning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of ML types\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Supervised Learning example\n",
    "X_super = np.random.randn(50, 2)\n",
    "y_super = (X_super[:, 0] + X_super[:, 1] > 0).astype(int)\n",
    "axes[0].scatter(X_super[:, 0], X_super[:, 1], c=y_super, cmap='viridis', s=50)\n",
    "axes[0].set_title('Supervised Learning\\n(Labeled Data)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Unsupervised Learning example\n",
    "X_unsuper = np.random.randn(50, 2)\n",
    "axes[1].scatter(X_unsuper[:, 0], X_unsuper[:, 1], c='gray', s=50)\n",
    "axes[1].set_title('Unsupervised Learning\\n(Unlabeled Data)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "# Reinforcement Learning concept\n",
    "axes[2].text(0.5, 0.7, 'Agent', ha='center', va='center', fontsize=14, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "axes[2].text(0.5, 0.3, 'Environment', ha='center', va='center', fontsize=14,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "axes[2].annotate('Action', xy=(0.5, 0.65), xytext=(0.5, 0.35),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "axes[2].annotate('Reward', xy=(0.5, 0.35), xytext=(0.5, 0.65),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "axes[2].set_title('Reinforcement Learning\\n(Agent-Environment)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='preprocessing'></a>\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Data preprocessing is crucial for building effective ML models. Common steps include:\n",
    "\n",
    "1. **Handling Missing Values**\n",
    "2. **Feature Scaling**\n",
    "3. **Encoding Categorical Variables**\n",
    "4. **Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "data = {\n",
    "    'age': [25, 30, 35, np.nan, 45, 50, 28, 33, 40, 38],\n",
    "    'salary': [50000, 60000, 70000, 65000, 80000, 90000, 55000, 62000, 75000, 72000],\n",
    "    'department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR', 'Finance', 'IT', 'HR', 'Finance'],\n",
    "    'performance': [1, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with mean\n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nAfter handling missing values:\")\n",
    "print(df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Scaling\n",
    "\n",
    "Scaling ensures all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "df[['age_scaled', 'salary_scaled']] = scaler.fit_transform(df[['age', 'salary']])\n",
    "\n",
    "print(\"Original vs Scaled Features:\")\n",
    "print(df[['age', 'age_scaled', 'salary', 'salary_scaled']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['department'], prefix='dept')\n",
    "\n",
    "print(\"Dataset with Encoded Categories:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train-Test Split\n",
    "\n",
    "Split data into training and testing sets to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded.drop(['performance', 'age', 'salary'], axis=1)\n",
    "y = df_encoded['performance']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='linear-regression'></a>\n",
    "## 4. Linear Regression\n",
    "\n",
    "Linear Regression is used for predicting continuous values. It finds the best-fit line through the data.\n",
    "\n",
    "**Equation**: $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "Where:\n",
    "- $y$ = predicted value\n",
    "- $\\beta_0$ = intercept\n",
    "- $\\beta_i$ = coefficients\n",
    "- $x_i$ = features\n",
    "- $\\epsilon$ = error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample regression data\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) * 10\n",
    "y_reg = 2 * X_reg + 5 + np.random.randn(100, 1) * 2\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = lr_model.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Model Coefficients: {lr_model.coef_[0][0]:.2f}\")\n",
    "print(f\"Model Intercept: {lr_model.intercept_[0]:.2f}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear Regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test_reg, y_test_reg, color='blue', alpha=0.6, label='Actual')\n",
    "plt.plot(X_test_reg, y_pred_reg, color='red', linewidth=2, label='Predicted')\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.title('Linear Regression: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='logistic-regression'></a>\n",
    "## 5. Logistic Regression\n",
    "\n",
    "Logistic Regression is used for binary classification problems. It predicts the probability of an instance belonging to a class.\n",
    "\n",
    "**Sigmoid Function**: $P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$\n",
    "\n",
    "Output is between 0 and 1 (probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=200, n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=42, n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_class = log_reg.predict(X_test_class)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_class)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Decision Boundary\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis', s=50)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_test_class, y_test_class, log_reg, \n",
    "                      'Logistic Regression Decision Boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='decision-trees'></a>\n",
    "## 6. Decision Trees\n",
    "\n",
    "Decision Trees are versatile models that can be used for both classification and regression. They split the data based on features to create a tree-like structure.\n",
    "\n",
    "**Advantages**:\n",
    "- Easy to understand and interpret\n",
    "- Handles both numerical and categorical data\n",
    "- Requires little data preprocessing\n",
    "\n",
    "**Disadvantages**:\n",
    "- Prone to overfitting\n",
    "- Can be unstable with small variations in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_classifier.predict(X_test_class)\n",
    "\n",
    "# Evaluate\n",
    "dt_accuracy = accuracy_score(y_test_class, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_class, y_pred_dt))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Decision Tree boundary\n",
    "plot_decision_boundary(X_test_class, y_test_class, dt_classifier, \n",
    "                      'Decision Tree Decision Boundary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = dt_classifier.feature_importances_\n",
    "feature_names = [f'Feature {i+1}' for i in range(X_class.shape[1])]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(feature_names, feature_importance, color='teal')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance in Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='knn'></a>\n",
    "## 7. K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a simple, instance-based learning algorithm. It classifies a data point based on the majority class of its k nearest neighbors.\n",
    "\n",
    "**Key Points**:\n",
    "- Non-parametric (no assumptions about data distribution)\n",
    "- Lazy learner (no training phase)\n",
    "- Distance-based algorithm\n",
    "\n",
    "**Common Distance Metrics**:\n",
    "- Euclidean Distance: $d = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$\n",
    "- Manhattan Distance: $d = \\sum_{i=1}^{n}|x_i - y_i|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn.predict(X_test_class)\n",
    "\n",
    "# Evaluate\n",
    "knn_accuracy = accuracy_score(y_test_class, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {knn_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KNN boundary\n",
    "plot_decision_boundary(X_test_class, y_test_class, knn, \n",
    "                      'KNN Decision Boundary (k=5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal K value\n",
    "k_values = range(1, 21)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_temp.fit(X_train_class, y_train_class)\n",
    "    y_pred_temp = knn_temp.predict(X_test_class)\n",
    "    accuracies.append(accuracy_score(y_test_class, y_pred_temp))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('K Value', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('KNN: Finding Optimal K Value', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "plt.axvline(x=best_k, color='red', linestyle='--', label=f'Best K = {best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best K value: {best_k} with accuracy: {max(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "## 8. Model Evaluation\n",
    "\n",
    "Proper model evaluation is crucial to understand performance and avoid overfitting.\n",
    "\n",
    "### 8.1 Regression Metrics\n",
    "- **Mean Squared Error (MSE)**: Average squared difference between predictions and actual values\n",
    "- **Root Mean Squared Error (RMSE)**: Square root of MSE\n",
    "- **R² Score**: Proportion of variance explained by the model (0 to 1)\n",
    "- **Mean Absolute Error (MAE)**: Average absolute difference\n",
    "\n",
    "### 8.2 Classification Metrics\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- **Precision**: True Positives / (True Positives + False Positives)\n",
    "- **Recall (Sensitivity)**: True Positives / (True Positives + False Negatives)\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall\n",
    "- **Confusion Matrix**: Table showing correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for comprehensive evaluation\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "dt_iris = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dt_iris.fit(X_train_iris, y_train_iris)\n",
    "y_pred_iris = dt_iris.predict(X_test_iris)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - Iris Dataset', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='cross-validation'></a>\n",
    "## 9. Cross-Validation\n",
    "\n",
    "Cross-validation is a technique to assess model performance more reliably by training and testing on different subsets of data.\n",
    "\n",
    "**K-Fold Cross-Validation**:\n",
    "1. Split data into K folds\n",
    "2. Train on K-1 folds, test on remaining fold\n",
    "3. Repeat K times\n",
    "4. Average the results\n",
    "\n",
    "**Benefits**:\n",
    "- More reliable performance estimate\n",
    "- Uses all data for training and testing\n",
    "- Reduces variance in performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-Fold Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Different models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_iris, y_iris, cv=5, scoring='accuracy')\n",
    "    results[name] = cv_scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  CV Scores: {cv_scores}\")\n",
    "    print(f\"  Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cross-Validation Results\n",
    "plt.figure(figsize=(10, 6))\n",
    "positions = np.arange(len(models))\n",
    "bp = plt.boxplot([results[name] for name in models.keys()], \n",
    "                  labels=models.keys(), patch_artist=True)\n",
    "\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Cross-Validation Results Comparison', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='feature-engineering'></a>\n",
    "## 10. Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating new features or transforming existing ones to improve model performance.\n",
    "\n",
    "**Common Techniques**:\n",
    "1. **Creating Interaction Features**: Combining multiple features\n",
    "2. **Polynomial Features**: Creating polynomial combinations\n",
    "3. **Binning**: Converting continuous variables to categorical\n",
    "4. **Feature Selection**: Choosing most relevant features\n",
    "5. **Domain-Specific Features**: Using domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Example: Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_iris_poly = poly.fit_transform(X_iris[:5])  # Transform first 5 samples for display\n",
    "\n",
    "print(\"Original Features (first 5 samples):\")\n",
    "print(X_iris[:5])\n",
    "print(f\"Original shape: {X_iris[:5].shape}\")\n",
    "print(\"\\nPolynomial Features (degree=2):\")\n",
    "print(X_iris_poly)\n",
    "print(f\"New shape: {X_iris_poly.shape}\")\n",
    "print(f\"\\nFeature names: {poly.get_feature_names_out(iris.feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection: Select K Best\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_iris_selected = selector.fit_transform(X_iris, y_iris)\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Score': selector.scores_\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Scores:\")\n",
    "print(feature_scores)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_scores['Feature'], feature_scores['Score'], color='coral')\n",
    "plt.xlabel('F-Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance for Iris Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Next Steps\n",
    "\n",
    "### What We Covered:\n",
    "1. Introduction to Machine Learning concepts\n",
    "2. Types of ML (Supervised, Unsupervised, Reinforcement)\n",
    "3. Data Preprocessing techniques\n",
    "4. Linear Regression for continuous predictions\n",
    "5. Logistic Regression for binary classification\n",
    "6. Decision Trees for interpretable models\n",
    "7. K-Nearest Neighbors for instance-based learning\n",
    "8. Model Evaluation metrics and techniques\n",
    "9. Cross-Validation for robust performance assessment\n",
    "10. Feature Engineering to improve model performance\n",
    "\n",
    "### Next Steps in Your ML Journey:\n",
    "1. **Advanced Algorithms**: Random Forests, Gradient Boosting (XGBoost, LightGBM), Support Vector Machines\n",
    "2. **Deep Learning**: Neural Networks, CNNs, RNNs\n",
    "3. **Unsupervised Learning**: K-Means, DBSCAN, PCA, t-SNE\n",
    "4. **Time Series Analysis**: ARIMA, LSTM\n",
    "5. **Natural Language Processing**: Text classification, sentiment analysis\n",
    "6. **Computer Vision**: Image classification, object detection\n",
    "7. **Model Deployment**: Flask, FastAPI, Docker, Cloud platforms\n",
    "8. **MLOps**: Model versioning, monitoring, CI/CD\n",
    "\n",
    "### Resources:\n",
    "- **Books**: \"Hands-On Machine Learning\" by Aurélien Géron, \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- **Online Courses**: Coursera, fast.ai, deeplearning.ai\n",
    "- **Practice**: Kaggle competitions, UCI ML Repository\n",
    "- **Documentation**: scikit-learn, TensorFlow, PyTorch\n",
    "\n",
    "### Practice Projects:\n",
    "1. House price prediction\n",
    "2. Customer churn prediction\n",
    "3. Image classification\n",
    "4. Sentiment analysis\n",
    "5. Recommendation systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Model comparison\n",
    "print(\"Congratulations on completing the ML Fundamentals notebook!\")\n",
    "print(\"Keep practicing and exploring more advanced topics.\")\n",
    "print(\"Remember: The key to mastering ML is consistent practice and experimentation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
